{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5b70e78",
   "metadata": {},
   "source": [
    "# Women's Crime data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f625920",
   "metadata": {},
   "source": [
    "Column Description\n",
    "There are 20 districts in the given data based on which there are 20 rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5574c7",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a681c9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.clustering.KMeans\r\n",
       "import org.apache.spark.ml.evaluation.ClusteringEvaluator\r\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.clustering.KMeans\n",
    "import org.apache.spark.ml.evaluation.ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea6f42ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\r\n",
       "import org.apache.spark.ml.classification.DecisionTreeClassifier\r\n",
       "import org.apache.spark.ml.classification.DecisionTreeClassificationModel\r\n",
       "import org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorIndexer}\r\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\r\n",
       "import org.apache.spark.mllib.util.MLUtils\r\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassificationModel\n",
    "import org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorIndexer}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.mllib.util.MLUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16c56ccb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-KKJ5F40:4041\n",
       "SparkContext available as 'sc' (version = 3.1.2, master = local[*], app id = local-1635364267229)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+---+---+---+-----+----+---+---+----+----+----+-----+----+----+----+----+----+----+----+-----+\n",
      "|_c0| _c1|_c2|_c3|_c4|_c5|  _c6| _c7|_c8|_c9|_c10|_c11|_c12| _c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20| _c21|\n",
      "+---+----+---+---+---+---+-----+----+---+---+----+----+----+-----+----+----+----+----+----+----+----+-----+\n",
      "|  5| 111|371|  8|  4|  1| 6546| 534| 64|  0|   0|1095| 141| 4886|2342| 435|  48|   0|  49| 449|   0|17089|\n",
      "|  0|   0|  2|  0|  0|  0|   69|  45|  1|  0|   0|  60|   2|   73|   3|   0|   0|   0|   0|  26|   0|  281|\n",
      "| 26| 148| 52| 29|  4|  2|11408|5364| 66|  1|   0|1657| 485| 4642|  90| 469|  18|   0| 395|1496|   0|26352|\n",
      "|  3|1046| 21|  1|  1|  0| 1935|6671| 15|  1|   0| 806|  69|  584|   6|2555|  30|  23|   9|1583|   0|15359|\n",
      "|  8|  71|150|  1|  1|  0|  641|1341| 21|  0|   0|1210| 118| 1461| 279|   6|   3|   0|  36|2038|   0| 7385|\n",
      "|  1|   0|  0|  0|  0|  0|    8|  31| 16|  0|   0|  60|   0|   67|  35|   0|   0|   0|   1|   0|   0|  219|\n",
      "|  7|   6|295|  0|  4|  8| 3345| 597|  4|  2|   0| 486|   3|  846|  17|   2|  47|   0|  27|2330|   2| 8028|\n",
      "|  2| 251|204|  7|  1|  0| 4119|2423|  9|  1|   0|1373| 215| 2339| 183|  25|  28|   0|  82|1738|   0|13000|\n",
      "|  8|   1| 57|  2|  0|  0|  259| 281|  0|  0|   0| 331|   3|  538|  88|   2|   0|   2|  38|   4|   0| 1614|\n",
      "|  9| 275| 51|  8|  1|  0|  857| 993| 46|  1|   0|1321| 188| 1358|   6|1518|   2|  66|   3| 927|   0| 7630|\n",
      "|  7| 176|257|  2|  5|  1| 2055| 923| 12|  0|   0| 504|  11| 4751|  70|1511| 159|   0| 143|2092|   1|12680|\n",
      "|  1|   6| 34|  0|  4|  2| 2707| 151| 72|  0|   0| 637|  59| 3890| 442|   3|   5| 165| 119|1839|   3|10139|\n",
      "| 27| 608|718| 37|  5|  4| 5540|4782| 34|  4|   1|2339|  35| 5378| 200|  90|  14| 180|  62|5582|   0|25640|\n",
      "| 20| 197|832| 18|  4|  1| 6729|5254|116|  0|   0|2061|   3| 9965| 969|  25|  85|   3| 102|5570|   0|31954|\n",
      "|  2|   1|  2|  0|  0|  0|    7|  49|  2|  0|   0|  32|   5|   61|   8|   0|   0|   1|   3|  74|   0|  247|\n",
      "|  2|   1|  0|  0|  2|  0|   17|  29|  0|  0|   0|  67|  19|   86|  16|   0|   0|   0|   9| 320|   0|  568|\n",
      "|  0|   0|  0|  0|  0|  0|    7|   1|  0|  0|   0|  33|   1|   26|   3|   0|   0|   0|   1| 100|   0|  172|\n",
      "|  0|   1|  0|  0|  0|  0|    2|   6|  0|  0|   0|   4|   0|    7|   2|   0|   0|   0|   0|  17|   0|   39|\n",
      "| 18| 320| 16|  2|  6|  0| 3659|3775| 56|  0|   0|1211| 144|12605| 615| 312|  19|   0| 526|2202|   3|25489|\n",
      "|  5|  63|176|  7|  2|  1| 1271|1241|  2|  0|   0| 502|  53|  731|  28|   7|  11|   0|  41| 697|   0| 4838|\n",
      "+---+----+---+---+---+---+-----+----+---+---+----+----+----+-----+----+----+----+----+----+----+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 20 more fields]\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df = spark.read.option(\"header\", false).csv(\"crime.csv\")\n",
    "   df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e217d225",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cols: Array[String] = Array(_c0, _c1, _c2, _c3, _c4, _c5, _c6, _c7, _c8, _c9, _c10, _c11, _c12, _c13, _c14, _c15, _c16, _c17, _c18, _c19, _c20, _c21)\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cols = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1055cd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the schema of the data frame: \n",
      "root\n",
      " |-- State: string (nullable = true)\n",
      " |-- Murder with Rape/Gang Rape: string (nullable = true)\n",
      " |-- Dowry Deaths: string (nullable = true)\n",
      " |-- Abetment to suicide of women: string (nullable = true)\n",
      " |-- Miscarriage: string (nullable = true)\n",
      " |-- Acid Attack: string (nullable = true)\n",
      " |-- Attempt to acid attack: string (nullable = true)\n",
      " |-- Cruelty by husband or his relatives: string (nullable = true)\n",
      " |-- Kidnapping or abduction of women: string (nullable = true)\n",
      " |-- Human Trafficking: string (nullable = true)\n",
      " |-- Selling of Minors: string (nullable = true)\n",
      " |-- Buying of Minor: string (nullable = true)\n",
      " |-- Rape: string (nullable = true)\n",
      " |-- Attempt to commit rape: string (nullable = true)\n",
      " |-- Assualt on women with intent to outrage her modesty: string (nullable = true)\n",
      " |-- Insult to the Modesty of women: string (nullable = true)\n",
      " |-- Dowry prohibition Act: string (nullable = true)\n",
      " |-- Immoral traffic prevention: string (nullable = true)\n",
      " |-- Protection of women from domestic violence Act: string (nullable = true)\n",
      " |-- Cyber Crime: string (nullable = true)\n",
      " |-- POCSO Act: string (nullable = true)\n",
      " |-- Incedent Representation Of Women: string (nullable = true)\n",
      " |-- Total: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"Checking the schema of the data frame: \")\n",
    "    df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90533f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.DoubleType\r\n",
       "df2: org.apache.spark.sql.DataFrame = [_c0: double, _c1: double ... 20 more fields]\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.DoubleType\n",
    "val df2 = df.withColumn(\"_c0\",col(\"_c0\").cast(DoubleType)).withColumn(\"_c1\",col(\"_c1\").cast(DoubleType)).withColumn(\"_c2\",col(\"_c2\").cast(DoubleType))\n",
    ".withColumn(\"_c3\",col(\"_c3\").cast(DoubleType)).withColumn(\"_c4\",col(\"_c4\").cast(DoubleType)).withColumn(\"_c5\",col(\"_c5\").cast(DoubleType))\n",
    ".withColumn(\"_c6\",col(\"_c6\").cast(DoubleType)).withColumn(\"_c7\",col(\"_c7\").cast(DoubleType)).withColumn(\"_c8\",col(\"_c8\").cast(DoubleType))\n",
    ".withColumn(\"_c9\",col(\"_c9\").cast(DoubleType)).withColumn(\"_c10\",col(\"_c10\").cast(DoubleType)).withColumn(\"_c11\",col(\"_c11\").cast(DoubleType))\n",
    ".withColumn(\"_c12\",col(\"_c12\").cast(DoubleType)).withColumn(\"_c13\",col(\"_c13\").cast(DoubleType)).withColumn(\"_c14\",col(\"_c14\").cast(DoubleType))\n",
    ".withColumn(\"_c15\",col(\"_c15\").cast(DoubleType)).withColumn(\"_c16\",col(\"_c16\").cast(DoubleType)).withColumn(\"_c17\",col(\"_c17\").cast(DoubleType))\n",
    ".withColumn(\"_c18\",col(\"_c18\").cast(DoubleType)).withColumn(\"_c19\",col(\"_c19\").cast(DoubleType)).withColumn(\"_c20\",col(\"_c20\").cast(DoubleType))\n",
    ".withColumn(\"_c21\",col(\"_c21\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9094cb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.DoubleType\r\n",
       "df2: org.apache.spark.sql.DataFrame = [_c0: double, _c1: double ... 21 more fields]\r\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.DoubleType\n",
    "//val df2 = df.withColumn(\"_c0\",col(\"_c0\").cast(DoubleType)).withColumn(\"_c1\",col(\"_c1\").cast(DoubleType)).withColumn(\"_c2\",col(\"_c2\").cast(DoubleType))\n",
    "//.withColumn(\"_c3\",col(\"_c3\").cast(DoubleType)).withColumn(\"_c4\",col(\"_c4\").cast(DoubleType)).withColumn(\"_c5\",col(\"_c5\").cast(DoubleType))\n",
    "//val df3 = df2.withColumn(\"_c5\",col(\"_c5\").cast(DoubleType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ae12b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\r\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f905ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_2d5798c6e518, handleInvalid=error, numInputCols=5\r\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().setInputCols(Array(\"_c1\",\"_c2\",\"_c3\",\"_c4\",\"_c5\")).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2d3e9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembled_data: org.apache.spark.sql.DataFrame = [_c0: double, _c1: double ... 22 more fields]\r\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembled_data = assembler.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c468ce0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StandardScaler\r\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc8f704f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_bb3f79e99212\r\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Scaler = new StandardScaler().\n",
    "      setInputCol(\"features\").setOutputCol(\"standardized\").\n",
    "      setWithStd(true).setWithMean(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e079c4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 8) (DESKTOP-KKJ5F40 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$3454/2355791: (struct<_c1:double,_c2:double,_c3:double,_c4:double,_c5:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 8) (DESKTOP-KKJ5F40 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$3454/2355791: (struct<_c1:double,_c2:double,_c3:double,_c4:double,_c5:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:87)\r",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)\r",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\r",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\r",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r",
      "\tat java.lang.Thread.run(Unknown Source)\r",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\r",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\r",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\r",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\r",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\r",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\r",
      "\t... 23 more\r",
      "\r",
      "Driver stacktrace:\r",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r",
      "  at scala.Option.foreach(Option.scala:407)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r",
      "  at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\r",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2729)\r",
      "  at org.apache.spark.sql.Dataset.first(Dataset.scala:2736)\r",
      "  at org.apache.spark.ml.feature.StandardScaler.fit(StandardScaler.scala:113)\r",
      "  ... 41 elided\r",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$3454/2355791: (struct<_c1:double,_c2:double,_c3:double,_c4:double,_c5:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r",
      "  at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:87)\r",
      "  at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\r",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r",
      "  at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r",
      "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r",
      "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:131)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r",
      "  ... 1 more\r",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\r",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r",
      "  at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\r",
      "  at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\r",
      "  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r",
      "  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r",
      "  at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\r",
      "  at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\r",
      "  at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\r",
      "  ... 23 more\r",
      ""
     ]
    }
   ],
   "source": [
    "val scale_data = Scaler.fit(assembled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3d4e274f",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "53: error: not found: value scale_data\r",
     "output_type": "error",
     "traceback": [
      "<console>:53: error: not found: value scale_data\r",
      "       val data_scale_output = scale_data.transform(assembled_data)\r",
      "                               ^\r",
      ""
     ]
    }
   ],
   "source": [
    "val data_scale_output = scale_data.transform(assembled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e23af0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kmeans: org.apache.spark.ml.clustering.KMeans = kmeans_afb86f5ffe36\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kmeans = new KMeans().setK(2).setSeed(1L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc8489b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.clustering.KMeansModel = KMeansModel: uid=kmeans_afb86f5ffe36, k=2, distanceMeasure=euclidean, numFeatures=5\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = kmeans.fit(assembled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd8a799d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [_c0: double, _c1: double ... 44 more fields]\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = model.transform(assembled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4319a90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----+----+---+---+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+--------------------+----------+\n",
      "| _c0|_c1| _c2| _c3|_c4|_c5|_c6|_c7|_c8|_c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|_c21|_c22|_c23|_c24|_c25|_c26|_c27|_c28|_c29|_c30|_c31|_c32|_c33|_c34|_c35|_c36|_c37|_c38|_c39|_c40|_c41|_c42|_c43|            features|prediction|\n",
      "+----+---+----+----+---+---+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+--------------------+----------+\n",
      "| 1.0|1.0|17.0|48.0|0.0|0.0|  0|437| 44| 17|  23|   0|   0|   3|   0|   0|  69|  32| 418| 387|  31|  26|  26|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   1|  33|  30|   3|   0|   0|   0|   0|   0|1129|[1.0,17.0,48.0,0....|         1|\n",
      "| 2.0|0.0|14.0|32.0|0.0|0.0|  0|335| 13|  6|   2|   0|   5|   0|   0|   0|  38|   9| 183| 147|  36|  12|  12|   0|   0|   4|   0|   0|   0|   0|   4|   0|   1|   0|   1|   6|   1|   3|   2|   0|   0|   0|   0| 647|(5,[1,2],[14.0,32...|         1|\n",
      "| 3.0|1.0| 7.0| 9.0|0.0|0.0|  0|283| 11|  6|   5|   0|   0|   0|   0|   0|  30|   4| 205| 196|   9|  66|  66|   0|  21|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|  13|   6|   6|   1|   0|   0|   0|   0| 650|[1.0,7.0,9.0,0.0,...|         0|\n",
      "| 4.0|0.0| 7.0|26.0|0.0|0.0|  0|405| 48| 10|   2|   0|   4|   0|   0|   0| 111|  10| 463| 425|  38|  63|  62|   1|   2|   0|   0|   0|   0|   0|   0|   0|   2|   2|   0|  62|  43|   4|   8|   0|   7|   0|   0|1199|(5,[1,2],[7.0,26.0])|         1|\n",
      "| 5.0|0.0| 0.0| 3.0|0.0|0.0|  0|  1|  0|  0|   0|   0|   0|   0|   0|   0|   1|   0|   3|   3|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   1|   0|   0|   0|   0|   0|   0|   9|       (5,[2],[3.0])|         0|\n",
      "| 6.0|0.0| 3.0|28.0|0.0|0.0|  1|614| 59| 10|   5|   0|   0|   2|   0|   0|  90|  12| 338| 292|  46| 262| 260|   2|   0|   4|   0|   0|   0|   0|   4|   0|   2|   0|   2|   0|   0|   0|   0|   0|   0|   0|   0|1415|(5,[1,2],[3.0,28.0])|         1|\n",
      "| 7.0|1.0| 5.0|19.0|0.0|0.0|  0|275| 34|  5|   2|   0|   2|  21|   0|   0|  52|   1| 198| 160|  38| 151| 150|   1|   0|  12|  10|   0|   0|   0|   2|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0| 769|[1.0,5.0,19.0,0.0...|         0|\n",
      "| 8.0|0.0| 8.0|25.0|0.0|0.0|  0|617|107|  5|   0|   0|   0|   0|   0|   0|  79|  14| 341| 305|  36| 242| 242|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   3|   0|   0|   0|   0|   3|   0|   0|1436|(5,[1,2],[8.0,25.0])|         1|\n",
      "| 9.0|0.0| 5.0|44.0|8.0|0.0|  0|222| 25|  9|   9|   0|   7|   0|   0|   0|  41|   8| 426| 426|   0| 471| 471|   0| 407|   9|   0|   0|   0|   0|   9|   0|   0|   0|   0|  24|  10|   8|   5|   0|   1|   0|   0|1690|[0.0,5.0,44.0,8.0...|         1|\n",
      "|10.0|0.0| 7.0|17.0|0.0|1.0|  0|433| 12|  5|   4|   0|   0|   2|   0|   0|  47|  15| 268| 189|  79|  65|  55|  10|   0|   6|   6|   0|   0|   0|   0|   0|   4|   4|   0|  72|  20|  49|   3|   0|   0|   0|   0| 949|[0.0,7.0,17.0,0.0...|         0|\n",
      "|11.0|1.0|10.0|26.0|0.0|2.0|  0|463|  6|  1|   0|   0|   1|   3|   0|   0|  31|  12| 280| 280|   0| 431| 429|   2|   0|   1|   0|   0|   0|   0|   1|   0|   2|   2|   0| 122|  66|  19|  37|   0|   0|   0|   0|1390|[1.0,10.0,26.0,0....|         1|\n",
      "|12.0|0.0| 2.0| 9.0|0.0|0.0|  0|225| 29|  7|   0|   0|   2|   8|   0|   0|  56|   1| 175| 158|  17|  83|  79|   4|   0|   1|   0|   0|   1|   0|   0|   0|   2|   1|   1|   2|   0|   1|   1|   0|   0|   0|   0| 593| (5,[1,2],[2.0,9.0])|         0|\n",
      "|13.0|0.0| 3.0|16.0|0.0|0.0|  0|153| 14|  6|   0|   0|   2|   0|   0|   0|  61|   0| 146| 132|  14|  64|  63|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0| 457|(5,[1,2],[3.0,16.0])|         0|\n",
      "|14.0|0.0| 0.0| 4.0|0.0|0.0|  0|129|  6|  2|   0|   0|   3|   0|   0|   0|  12|   1|  66|  61|   5|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   5|   3|   2|  18|  11|   5|   1|   1|   0|   0|   0| 241|       (5,[2],[4.0])|         0|\n",
      "|15.0|0.0| 7.0|15.0|0.0|0.0|  0|522| 14|  2|   0|   0|   2|   9|   0|   0|  62|   2| 225| 185|  40| 111| 109|   2|   0|   1|   1|   0|   0|   0|   0|   0|   2|   2|   0|   1|   0|   1|   0|   0|   0|   0|   0| 971|(5,[1,2],[7.0,15.0])|         0|\n",
      "|16.0|1.0| 0.0| 0.0|0.0|0.0|  0|  0|  1|  0|   0|   0|   1|   0|   0|   0|   1|   0|   2|   2|   0|   1|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   6|       (5,[0],[1.0])|         0|\n",
      "|17.0|0.0| 4.0|14.0|0.0|0.0|  0|134| 17|  0|   0|   0|   0|   0|   0|   0|  41|   3| 226| 206|  20|   1|   1|   0|   1|   0|   0|   0|   0|   0|   0|   0|   3|   3|   0|  20|  20|   0|   0|   0|   0|   0|   0| 464|(5,[1,2],[4.0,14.0])|         0|\n",
      "|18.0|0.0| 5.0|12.0|0.0|1.0|  0|423| 23|  0|   0|   0|   2|   8|   0|   0| 124|   2| 288| 269|  19| 104| 101|   3|   1|   1|   0|   0|   0|   0|   1|   0|  22|   1|  21|  21|  16|   2|   3|   0|   0|   0|   0|1035|[0.0,5.0,12.0,0.0...|         0|\n",
      "|19.0|0.0| 1.0| 5.0|0.0|0.0|  0|231| 26|  2|   0|   0|   0|   1|   0|   0|  73|   3| 178| 170|   8|  81|  81|   0|   2|   0|   0|   0|   0|   0|   0|   0|   3|   3|   0|  31|  27|   1|   3|   0|   0|   0|   0| 635| (5,[1,2],[1.0,5.0])|         0|\n",
      "|20.0|0.0| 6.0|19.0|0.0|0.0|  0|644| 45| 11|   1|   0|  18|   7|   0|   0|  76|  12| 457| 380|  77| 108| 101|   7|   1|   9|   0|   0|   3|   0|   6|   0|   0|   0|   0|  20|  20|   0|   0|   0|   0|   0|   0|1404|(5,[1,2],[6.0,19.0])|         0|\n",
      "+----+---+----+----+---+---+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06c6929f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.ClusteringEvaluator = ClusteringEvaluator: uid=cluEval_d6dcd3a77e31, metricName=silhouette, distanceMeasure=squaredEuclidean\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new ClusteringEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f37acbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.7137296038353063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "silhouette: Double = 0.7137296038353063\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val silhouette = evaluator.evaluate(predictions)\n",
    "println(s\"Silhouette with squared euclidean distance = $silhouette\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30c25229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[0.23076923076923078,3.6153846153846154,10.923076923076923,0.0,0.15384615384615385]\n",
      "[0.2857142857142857,9.142857142857142,32.714285714285715,1.1428571428571428,0.2857142857142857]\n"
     ]
    }
   ],
   "source": [
    "println(\"Cluster Centers: \")\n",
    "model.clusterCenters.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24feb186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[0.23076923076923078,3.6153846153846154,10.923076923076923,0.0,0.15384615384615385]\n",
      "[0.2857142857142857,9.142857142857142,32.714285714285715,1.1428571428571428,0.2857142857142857]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "centers: Array[org.apache.spark.ml.linalg.Vector] = Array([0.23076923076923078,3.6153846153846154,10.923076923076923,0.0,0.15384615384615385], [0.2857142857142857,9.142857142857142,32.714285714285715,1.1428571428571428,0.2857142857142857])\r\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Cluster Centers: \")\n",
    "val centers = model.clusterCenters\n",
    "centers.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09c27740",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "29: error: org.apache.spark.ml.evaluation.ClusteringEvaluator.type does not take parameters\r",
     "output_type": "error",
     "traceback": [
      "<console>:29: error: org.apache.spark.ml.evaluation.ClusteringEvaluator.type does not take parameters\r",
      "       val evaluator = ClusteringEvaluator(predictionCol=\"prediction\", featuresCol=\"standardized\", \\\r",
      "                                          ^\r",
      ""
     ]
    }
   ],
   "source": [
    "val evaluator = ClusteringEvaluator(predictionCol=\"prediction\", featuresCol=\"standardized\", \\\n",
    "                                metricName=\"silhouette\", distanceMeasure=\"squaredEuclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b6deeec",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "31: error: not found: value Kmeans_algo\r",
     "output_type": "error",
     "traceback": [
      "<console>:31: error: not found: value Kmeans_algo\r",
      "       val model = Kmeans_algo.fit(assembled_data)\r",
      "                   ^\r",
      ""
     ]
    }
   ],
   "source": [
    "val model = Kmeans_algo.fit(assembled_data)\n",
    "\n",
    "// Make predictions\n",
    "val predictions = model.transform(assembled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea159ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[String] = Array(_c0, _c1, _c2, _c3, _c4, _c5, _c6, _c7, _c8, _c9, _c10, _c11, _c12, _c13, _c14, _c15, _c16, _c17, _c18, _c19, _c20, _c21, _c22, _c23, _c24, _c25, _c26, _c27, _c28, _c29, _c30, _c31, _c32, _c33, _c34, _c35, _c36, _c37, _c38, _c39, _c40, _c41, _c42, _c43)\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845dd7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val assemble = new VectorAssembler().setInputCols(Array(\"_c0\",\"_c1\", \"_c2\",\"_c3\", \"_c4\", \"_c5\", \"_c6\", \"_c7\", \"_c8\", \"_c9\", \"_c10\", \"_c11\", \n",
    "                                      \"_c12\", \"_c13\", \"_c14\", \"_c15\", \"_c16\", \"_c17\", \"_c18\", \"_c19\", \"_c20\", \"_c21\", \"_c22\", \"_c23\", \"_c24\", \"_c25\", \n",
    "                                      \"_c26\", \"_c27\", \"_c28\", \"_c29\", \"_c30\", \"_c31\", \"_c32\", \"_c33\", \"_c34\", \"_c35\", \"_c36\", \"_c37\", \"_c38\", \"_c39\", \"_c40\", \"_c41\", \"_c42\", \"_c43\")).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a02740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
